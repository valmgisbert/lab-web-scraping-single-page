{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB | Web scraping multiple pages (pt 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice web scraping. This is not involved with the GNOD project of the week.** <br>\n",
    "Choose at least 3 sites to scrape:\n",
    "- Retrieve an arbitrary Wikipedia page of \"Python\" and create a list of links on that page: `url ='https://en.wikipedia.org/wiki/Python'`\n",
    "- Find the number of titles that have changed in the United States Code since its last release point: `url = 'http://uscode.house.gov/download/download.shtml'`\n",
    "- Create a Python list with the top ten FBI's Most Wanted names: `url = 'https://www.fbi.gov/wanted/topten'`\n",
    "- Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe: `url = 'https://www.emsc-csem.org/Earthquake/'`\n",
    "- List all language names and number of related articles in the order they appear in [wikipedia.org](wikipedia.org): `url = 'https://www.wikipedia.org/'`\n",
    "- A list with the different kind of datasets available in [data.gov.uk](data.gov.uk): `url = 'https://data.gov.uk/'`\n",
    "- Display the top 10 languages by number of native speakers stored in a pandas dataframe: `url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. TOP10 FBI's monst wanted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save url in var \n",
    "url = \"https://www.fbi.gov/wanted/topten\"\n",
    "\n",
    "# download html with get req\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make good soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALEJANDRO ROSALES CASTILLO',\n",
       " 'RUJA IGNATOVA',\n",
       " 'DONALD EUGENE FIELDS II',\n",
       " 'ALEXIS FLORES',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'WILVER VILLEGAS-PALOMINO',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract list from copied path\n",
    "most_wanted = []\n",
    "\n",
    "for person in soup.select(\"#query-results-0f737222c5054a81a120bce207b0446a > ul > li> h3 > a\"):\n",
    "    most_wanted.append(person.get_text())\n",
    "    \n",
    "most_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. All datasets available in `data.gov.uk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save url in var \n",
    "url = \"https://data.gov.uk/\"\n",
    "\n",
    "# download html with get req\n",
    "response=requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make good soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract list from copied path\n",
    "dataset_list = []\n",
    "\n",
    "for dset in soup.select(\"#main-content > div:nth-child(3) > div > ul > li > h3 > a\"):\n",
    "    dataset_list.append(dset.get_text())\n",
    "\n",
    "dataset_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Arbitrary Wiki page of \"Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save url in var \n",
    "url = \"https://en.wikipedia.org/wiki/Python\"\n",
    "\n",
    "# download html with get req\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make good soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list from copied path\n",
    "links = []\n",
    "\n",
    "for link in soup.select(\"#mw-content-text > div.mw-parser-output > ul > li > a\"):\n",
    "    url = \"https://en.wikipedia.org\" + link.get(\"href\")\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/Python_(genus)\" title=\"Python (genus)\"><i>Python</i> (genus)</a>,\n",
       " <a href=\"/wiki/Python_(Monty)_Pictures\" title=\"Python (Monty) Pictures\">Python (Monty) Pictures</a>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the missing links\n",
    "soup.select(\"#mw-content-text > div.mw-parser-output > ul > li > ul > li > a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.select('#mw-content-text > div.mw-parser-output > ul > li > ul > li > a'):\n",
    "    url = \"https://en.wikipedia.org\" + link.get(\"href\")\n",
    "    links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/Pyton'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = soup.select(\"#mw-content-text > div.mw-parser-output > ul > li > i > a\")\n",
    "a[0].get(\"href\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org/wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(codename)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Timon_of_Phlius',\n",
       " 'https://en.wikipedia.org/wiki/Pithon',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/wiki/Pyton']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.append(\"https://en.wikipedia.org\" + a[0].get(\"href\"))\n",
    "links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
